{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Admission_Predict.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eHtyhXm3TE6"
      },
      "source": [
        "# Imports libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdFI0ANjtVIp",
        "outputId": "6ee96bfb-9101-423f-cd47-c980023f73ac"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "sns.set_style('whitegrid')\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xlwt\n",
        "from xlwt import Workbook\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.stats.mstats import winsorize\n",
        "from pylab import rcParams\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import max_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "!pip install vowpalwabbit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vowpalwabbit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/f9/19792e9ce6b192e4b0475347c9c048f73c2bf2765a79fdf25bda7e78a26b/vowpalwabbit-8.10.2-cp37-cp37m-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: vowpalwabbit\n",
            "Successfully installed vowpalwabbit-8.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nreSqmO3f0Z"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATZ8dSqhH-AX"
      },
      "source": [
        "path = \"/content/sample_data/Admission_Predict_kaggle.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOQc7NhdIHfQ",
        "outputId": "69c038c1-7c53-4b0b-8056-b63a34259ad1"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eijyu9u6INID",
        "outputId": "ec804290-6b29-4610-bdb7-bebdf9ced01d"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n",
              "       'Research', 'Chance_of_Admit '],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDMxNEk8IRt1",
        "outputId": "38cd9eb7-8c9b-4f51-a25d-d2f2ac12a563"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 8 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   GRE Score          400 non-null    int64  \n",
            " 1   TOEFL Score        400 non-null    int64  \n",
            " 2   University Rating  400 non-null    int64  \n",
            " 3   SOP                400 non-null    float64\n",
            " 4   LOR                400 non-null    float64\n",
            " 5   CGPA               400 non-null    float64\n",
            " 6   Research           400 non-null    int64  \n",
            " 7   Chance_of_Admit    400 non-null    float64\n",
            "dtypes: float64(4), int64(4)\n",
            "memory usage: 25.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pBv6Mtq3t9n"
      },
      "source": [
        "# Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZQY7eV6N-5D"
      },
      "source": [
        "X = df.drop(['Chance_of_Admit '], axis=1)\n",
        "y = df['Chance_of_Admit ']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrLFzG6-Ynnq"
      },
      "source": [
        "# Workbook is created\n",
        "wb = Workbook()\n",
        "sheet1 = wb.add_sheet('Admission_Predict')\n",
        "sheet1.write(0, 1, 'Dataset name')\n",
        "sheet1.write(0, 2, 'Algorithm Name')\n",
        "sheet1.write(0, 3, 'Cross Validation [1-10]')\n",
        "sheet1.write(0, 4, 'Hyper-Parameters Values')\n",
        "sheet1.write(0, 5, 'Mean Squared Error')\n",
        "sheet1.write(0, 6, 'Mean Absolute Error')\n",
        "sheet1.write(0, 7, 'Median Absolute Error')\n",
        "sheet1.write(0, 8, 'Max Error')\n",
        "sheet1.write(0, 9, 'Explained Variance')\n",
        "sheet1.write(0, 10, 'Train Time')\n",
        "sheet1.write(0, 11, 'Inference Time')\n",
        "\n",
        "sheet1.write(1, 1, 'Admission_Predict_Kaggle')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OMWy_DC3zMS"
      },
      "source": [
        "# VWRegressor evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW9pfrfzNewC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e413be-a574-4082-d4b8-664315151676"
      },
      "source": [
        "from vowpalwabbit.sklearn_vw import VWRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# manual nested cross-validation for random forest on a classification dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import loguniform\n",
        "from scipy.stats import randint\n",
        "\n",
        "# configure the cross-validation procedure\n",
        "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# enumerate splits\n",
        "mean_squared_results = list()\n",
        "mean_absolute_results = list()\n",
        "median_absolute_results = list()\n",
        "max_err_results = list()\n",
        "explained_var_results = list()\n",
        "train_time = list()\n",
        "test_time = list()\n",
        "params = list()\n",
        "i = 1\n",
        "sheet1.write(1, 2, 'VWRegressor')\n",
        "for train_ix, test_ix in cv_outer.split(X):\n",
        "\t# split data\n",
        "\tX_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
        "\ty_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
        "\t# configure the cross-validation procedure\n",
        "\tcv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\t# define the model\n",
        "\tvw = VWRegressor()\n",
        "\t# define search space\n",
        "\tdistributions = {\n",
        "     'l1': loguniform(1e-8, 1e-1),\n",
        "     'l2': loguniform(1e-8, 1e-1),\n",
        "     'l': loguniform(0.01, 10),\n",
        "     'power_t': uniform(0.01, 1),\n",
        "     'random_weights': [\"on\", \"off\"],\n",
        "     'loss_function': [\"squared\", \"hinge\", \"logistic\", \"quantile\", \"poisson\"],\n",
        "     'passes': randint(1, 10),\n",
        "  }\n",
        "\t# define search\n",
        "\tclf = RandomizedSearchCV(vw, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\t# execute search\n",
        "\tsearch = clf.fit(X_train, y_train)\n",
        "\t# get the best performing model fit on the whole training set\n",
        "\tbest_model = search.best_estimator_\n",
        "\t# print(i)\n",
        "\t# print(best_model.get_params())\n",
        "\tparams.append(str(best_model.get_params()))\n",
        "\tstart = time.time()\n",
        "\tbest_model = best_model.fit(X_train, y_train)\n",
        "\tend = time.time()\n",
        "\ttrain = end - start\n",
        "\t# evaluate model on the hold out dataset\n",
        "\tstart = time.time()\n",
        "\tyhat = best_model.predict(X_test)\n",
        "\tend = time.time()\n",
        "\ttest = end - start\n",
        "\t# evaluate the model\n",
        "\tmean_squared = mean_squared_error(y_test, yhat)\n",
        "\tmean_absolute = mean_absolute_error(y_test, yhat)\n",
        "\tmedian_absolute = median_absolute_error(y_test, yhat)\n",
        "\tmax_err = max_error(y_test, yhat)\n",
        "\texplained_var = explained_variance_score(y_test, yhat)\n",
        "\t# store the result\n",
        "\tmean_squared_results.append(mean_squared)\n",
        "\tmean_absolute_results.append(mean_absolute)\n",
        "\tmedian_absolute_results.append(median_absolute)\n",
        "\tmax_err_results.append(max_err)\n",
        "\texplained_var_results.append(explained_var)\n",
        "\t# print('Mean Squared Error: %.3f' % (mean_squared_results))\n",
        "\t# print('Mean Absolute Error: %.3f' % (mean_absolute_results))\n",
        "\t# print('Median Absolute Error: %.3f' % (median_absolute_results))\n",
        "\t# print('Max Error: %.3f' % (median_absolute_results))\n",
        "\t# print('Explained Variance: %.3f' % (explained_var_results))\n",
        "\t# print('Train Time: %.3f' % (train_time))\n",
        "\t# print('Test Time: %.3f' % (test_time))\n",
        "\tsheet1.write(i, 3, i)\n",
        "\tsheet1.write(i, 4, str(best_model.get_params()))\n",
        "\tsheet1.write(i, 5, (mean_squared))\n",
        "\tsheet1.write(i, 6, (mean_absolute))\n",
        "\tsheet1.write(i, 7,  (median_absolute))\n",
        "\tsheet1.write(i, 8,  (max_err))\n",
        "\tsheet1.write(i, 9,  (explained_var))\n",
        "\tsheet1.write(i, 10, (train))\n",
        "\tsheet1.write(i, 11,  (test))\n",
        "\ti += 1\n",
        "\n",
        "# summarize the estimated performance of the model\n",
        "print('Mean Squared Error: %.3f (%.3f)' % (mean(mean_squared_results), std(mean_squared_results)))\n",
        "print('Mean Absolute Error: %.3f (%.3f)' % (mean(mean_absolute_results), std(mean_absolute_results)))\n",
        "print('Median Absolute Error: %.3f (%.3f)' % (mean(median_absolute_results), std(median_absolute_results)))\n",
        "print('Max Error: %.3f (%.3f)' % (mean(max_err_results), std(max_err_results)))\n",
        "print('Explained Variance: %.3f (%.3f)' % (mean(explained_var_results), std(explained_var_results)))\n",
        "print('Train Time: %.3f (%.3f)' % (mean(train_time), std(train_time)))\n",
        "print('Test Time: %.3f (%.3f)' % (mean(test_time), std(test_time)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 2.994 (0.069)\n",
            "Mean Absolute Error: 1.724 (0.020)\n",
            "Median Absolute Error: 1.729 (0.024)\n",
            "Max Error: 1.956 (0.018)\n",
            "Explained Variance: -0.000 (0.000)\n",
            "Train Time: nan (nan)\n",
            "Test Time: nan (nan)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHu6OlR4CwZ"
      },
      "source": [
        "# Suggestion for improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15U2bKpC4b65",
        "outputId": "5f1e9340-9bb7-4f3f-fe05-4a6d826d48bd"
      },
      "source": [
        "from vowpalwabbit.sklearn_vw import VWRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# manual nested cross-validation for random forest on a classification dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# configure the cross-validation procedure\n",
        "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# enumerate splits\n",
        "mean_squared_results = list()\n",
        "mean_absolute_results = list()\n",
        "median_absolute_results = list()\n",
        "max_err_results = list()\n",
        "explained_var_results = list()\n",
        "train_time = list()\n",
        "test_time = list()\n",
        "j = 1\n",
        "sheet1.write(11, 2, 'Suggestion for improvement')\n",
        "for train_ix, test_ix in cv_outer.split(X):\n",
        "\t# split data\n",
        "\tX_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
        "\ty_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
        "\t# configure the cross-validation procedure\n",
        "\tcv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\t# define the model\n",
        "\tvw_squared = VWRegressor(loss_function='squared')\n",
        "\tvw_hinge = VWRegressor(loss_function='hinge')\n",
        "\tvw_logistic = VWRegressor(loss_function='logistic')\n",
        "\tvw_quantile = VWRegressor(loss_function='quantile')\n",
        "\tvw_poisson = VWRegressor(loss_function='poisson')\n",
        "\t# define search space\n",
        "\tdistributions = {\n",
        "     'l1': loguniform(1e-8, 1e-1),\n",
        "     'l2': loguniform(1e-8, 1e-1),\n",
        "     'l': loguniform(0.01, 10),\n",
        "     'power_t': uniform(0.01, 1),\n",
        "     'random_weights': [\"on\", \"off\"],\n",
        "     'passes': randint(1, 10),\n",
        "\t}\n",
        "\t# define search\n",
        "\tclf_squared = RandomizedSearchCV(vw_squared, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\tclf_hinge = RandomizedSearchCV(vw_hinge, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\tclf_logistic = RandomizedSearchCV(vw_logistic, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\tclf_quantile = RandomizedSearchCV(vw_quantile, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\tclf_poisson = RandomizedSearchCV(vw_poisson, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\t# execute search\n",
        "\tsearch_squared = clf_squared.fit(X_train, y_train)\n",
        "\tsearch_hinge = clf_hinge.fit(X_train, y_train)\n",
        "\tsearch_logistic = clf_logistic.fit(X_train, y_train)\n",
        "\tsearch_quantile = clf_quantile.fit(X_train, y_train)\n",
        "\tsearch_poisson = clf_poisson.fit(X_train, y_train)\n",
        "\t# get the best performing model fit on the whole training set\n",
        "\tbest_model_squared = search_squared.best_estimator_\n",
        "\tbest_model_hinge = search_hinge.best_estimator_\n",
        "\tbest_model_logistic = search_logistic.best_estimator_\n",
        "\tbest_model_quantile = search_quantile.best_estimator_\n",
        "\tbest_model_poisson = search_poisson.best_estimator_\n",
        "\t# print(j)\n",
        "\t# print(best_model_squared.get_params())\n",
        "\t# print(best_model_hinge.get_params())\n",
        "\t# print(best_model_logistic.get_params())\n",
        "\t# print(best_model_quantile.get_params())\n",
        "\t# print(best_model_poisson.get_params())\n",
        "\t# fit the model\n",
        "\tstart = time.time()\n",
        "\tbest_model_squared = best_model_squared.fit(X_train, y_train)\n",
        "\tbest_model_hinge = best_model_hinge.fit(X_train, y_train)\n",
        "\tbest_model_logistic = best_model_logistic.fit(X_train, y_train)\n",
        "\tbest_model_quantile = best_model_quantile.fit(X_train, y_train)\n",
        "\tbest_model_poisson = best_model_poisson.fit(X_train, y_train)\n",
        "\tend = time.time()\n",
        "\ttrain = end - start\n",
        "\t# evaluate model on the hold out dataset\n",
        "\tstart = time.time()\n",
        "\tyhat_squared = best_model_squared.predict(X_test)\n",
        "\tyhat_hinge = best_model_hinge.predict(X_test)\n",
        "\tyhat_logistic = best_model_logistic.predict(X_test)\n",
        "\tyhat_quantile = best_model_quantile.predict(X_test)\n",
        "\tyhat_poisson = best_model_poisson.predict(X_test)\n",
        "\tavg = []\n",
        "\tfor i in range(len(yhat_squared)):\n",
        "\t\tavg.append((yhat_squared[i] + yhat_hinge[i] + yhat_logistic[i] + yhat_quantile[i] + yhat_poisson[i]) / 5)\n",
        "\tend = time.time()\n",
        "\ttest = end - start\n",
        "\t# evaluate the model\n",
        "\tmean_squared = mean_squared_error(y_test, avg)\n",
        "\tmean_absolute = mean_absolute_error(y_test, avg)\n",
        "\tmedian_absolute = median_absolute_error(y_test, avg)\n",
        "\tmax_err = max_error(y_test, avg)\n",
        "\texplained_var = explained_variance_score(y_test, avg)\n",
        "\t# store the result\n",
        "\tmean_squared_results.append(mean_squared)\n",
        "\tmean_absolute_results.append(mean_absolute)\n",
        "\tmedian_absolute_results.append(median_absolute)\n",
        "\tmax_err_results.append(max_err)\n",
        "\texplained_var_results.append(explained_var)\n",
        "\tsheet1.write(j + 10, 3, j)\n",
        "\tsheet1.write(j + 10, 4, str(best_model_squared.get_params()))\n",
        "\tsheet1.write(j + 10, 5, (mean_squared))\n",
        "\tsheet1.write(j + 10, 6,  (mean_absolute))\n",
        "\tsheet1.write(j + 10, 7, (median_absolute))\n",
        "\tsheet1.write(j + 10, 8, (max_err))\n",
        "\tsheet1.write(j + 10, 9, (explained_var))\n",
        "\tsheet1.write(j + 10, 10, (train))\n",
        "\tsheet1.write(j + 10, 11, (test))\n",
        "\tj += 1\n",
        "\n",
        "# summarize the estimated performance of the model\n",
        "print('Mean Squared Error: %.3f (%.3f)' % (mean(mean_squared_results), std(mean_squared_results)))\n",
        "print('Mean Absolute Error: %.3f (%.3f)' % (mean(mean_absolute_results), std(mean_absolute_results)))\n",
        "print('Median Absolute Error: %.3f (%.3f)' % (mean(median_absolute_results), std(median_absolute_results)))\n",
        "print('Max Error: %.3f (%.3f)' % (mean(max_err_results), std(max_err_results)))\n",
        "print('Explained Variance: %.3f (%.3f)' % (mean(explained_var_results), std(explained_var_results)))\n",
        "print('Train Time: %.3f (%.3f)' % (mean(train_time), std(train_time)))\n",
        "print('Test Time: %.3f (%.3f)' % (mean(test_time), std(test_time)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 454.748 (0.867)\n",
            "Mean Absolute Error: 21.324 (0.020)\n",
            "Median Absolute Error: 21.329 (0.024)\n",
            "Max Error: 21.556 (0.018)\n",
            "Explained Variance: -0.000 (0.000)\n",
            "Train Time: nan (nan)\n",
            "Test Time: nan (nan)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJvjRMkL4XEh"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7EbHYh15fC3",
        "outputId": "38d47d5a-92a5-4cd7-e069-69e6251e76b9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# manual nested cross-validation for random forest on a classification dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# configure the cross-validation procedure\n",
        "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# enumerate splits\n",
        "mean_squared_results = list()\n",
        "mean_absolute_results = list()\n",
        "median_absolute_results = list()\n",
        "max_err_results = list()\n",
        "explained_var_results = list()\n",
        "train_time = list()\n",
        "test_time = list()\n",
        "i = 1\n",
        "sheet1.write(21, 2, 'Random forest')\n",
        "for train_ix, test_ix in cv_outer.split(X):\n",
        "\t# split data\n",
        "\tX_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
        "\ty_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
        "\t# configure the cross-validation procedure\n",
        "\tcv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\t# define the model\n",
        "\trf = RandomForestRegressor()\n",
        "\t# define search space\n",
        "\tdistributions = {\n",
        "     'criterion': ['mse', 'mae'],\n",
        "     'min_samples_split': [2, 3, 4, 5],\n",
        "     'min_samples_leaf': [1, 2, 3, 4],\n",
        "     'min_weight_fraction_leaf': uniform(0.01, 0.5),\n",
        "     'max_features': ['auto', 'sqrt', 'log2']\n",
        "  }\n",
        "\t# define search\n",
        "\tclf = RandomizedSearchCV(rf, distributions, random_state=0, cv=cv_inner, n_iter=50)\n",
        "\t# execute search\n",
        "\tsearch = clf.fit(X_train, y_train)\n",
        "\t# get the best performing model fit on the whole training set\n",
        "\tbest_model = search.best_estimator_\n",
        "\t# print(i)\n",
        "\t# print(best_model.get_params())\n",
        "\tstart = time.time()\n",
        "\tbest_model = best_model.fit(X_train, y_train)\n",
        "\tend = time.time()\n",
        "\ttrain = end - start\n",
        "\t# evaluate model on the hold out dataset\n",
        "\tstart = time.time()\n",
        "\tyhat = best_model.predict(X_test)\n",
        "\tend = time.time()\n",
        "\ttest = end - start\n",
        "\t# evaluate the model\n",
        "\tmean_squared = mean_squared_error(y_test, yhat)\n",
        "\tmean_absolute = mean_absolute_error(y_test, yhat)\n",
        "\tmedian_absolute = median_absolute_error(y_test, yhat)\n",
        "\tmax_err = max_error(y_test, yhat)\n",
        "\texplained_var = explained_variance_score(y_test, yhat)\n",
        "\t# store the result\n",
        "\tmean_squared_results.append(mean_squared)\n",
        "\tmean_absolute_results.append(mean_absolute)\n",
        "\tmedian_absolute_results.append(median_absolute)\n",
        "\tmax_err_results.append(max_err)\n",
        "\texplained_var_results.append(explained_var)\n",
        "\tsheet1.write(i + 20, 3, i)\n",
        "\tsheet1.write(i + 20, 4, str(best_model_squared.get_params()))\n",
        "\tsheet1.write(i + 20, 5, (mean_squared))\n",
        "\tsheet1.write(i + 20, 6, (mean_absolute))\n",
        "\tsheet1.write(i + 20, 7, (median_absolute))\n",
        "\tsheet1.write(i + 20, 8, (max_err))\n",
        "\tsheet1.write(i + 20, 9, (explained_var))\n",
        "\tsheet1.write(i + 20, 10, (train))\n",
        "\tsheet1.write(i + 20, 11, (test))\n",
        "\ti += 1\n",
        "\n",
        "wb.save('Admission_Predict.xls')\n",
        "# summarize the estimated performance of the model\n",
        "print('Mean Squared Error: %.3f (%.3f)' % (mean(mean_squared_results), std(mean_squared_results)))\n",
        "print('Mean Absolute Error: %.3f (%.3f)' % (mean(mean_absolute_results), std(mean_absolute_results)))\n",
        "print('Median Absolute Error: %.3f (%.3f)' % (mean(median_absolute_results), std(median_absolute_results)))\n",
        "print('Max Error: %.3f (%.3f)' % (mean(max_err_results), std(max_err_results)))\n",
        "print('Explained Variance: %.3f (%.3f)' % (mean(explained_var_results), std(explained_var_results)))\n",
        "print('Train Time: %.3f (%.3f)' % (mean(train_time), std(train_time)))\n",
        "print('Test Time: %.3f (%.3f)' % (mean(test_time), std(test_time)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.005 (0.001)\n",
            "Mean Absolute Error: 0.048 (0.006)\n",
            "Median Absolute Error: 0.034 (0.005)\n",
            "Max Error: 0.210 (0.042)\n",
            "Explained Variance: 0.781 (0.053)\n",
            "Train Time: nan (nan)\n",
            "Test Time: nan (nan)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}